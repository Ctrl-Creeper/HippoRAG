"""
Ollama Qwen æœ¬åœ°æµ‹è¯• - å¿«é€Ÿå¯åŠ¨æŒ‡å—

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºå¦‚ä½•åœ¨ä½ çš„ç¯å¢ƒä¸­å¿«é€Ÿè¿è¡Œæµ‹è¯•
"""

# ============================================================================
# å‰ç½®æ¡ä»¶æ£€æŸ¥
# ============================================================================

def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    import subprocess
    import requests
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     å‰ç½®æ¡ä»¶æ£€æŸ¥                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ£€æŸ¥ 1: Ollama æœåŠ¡
    print("1ï¸âƒ£  æ£€æŸ¥ Ollama æœåŠ¡...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            print("   âœ… Ollama æœåŠ¡è¿è¡Œä¸­")
            models = response.json().get("models", [])
            print(f"   å¯ç”¨æ¨¡å‹:")
            for model in models:
                print(f"     - {model['name']}")
        else:
            print("   âŒ Ollama æœåŠ¡è¿”å›é”™è¯¯")
            return False
    except:
        print("   âŒ æ— æ³•è¿æ¥åˆ° Ollama (http://localhost:11434)")
        print("   ğŸ’¡ è¯·è¿è¡Œ: ollama serve")
        return False
    
    # æ£€æŸ¥ 2: Qwen æ¨¡å‹
    print("\n2ï¸âƒ£  æ£€æŸ¥ Qwen3:1.7b æ¨¡å‹...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        models = response.json().get("models", [])
        model_names = [m["name"] for m in models]
        
        if any("qwen" in m.lower() for m in model_names):
            print("   âœ… Qwen æ¨¡å‹å·²å®‰è£…")
        else:
            print("   âŒ æœªæ‰¾åˆ° Qwen æ¨¡å‹")
            print("   ğŸ’¡ è¯·è¿è¡Œ: ollama pull qwen3:1.7b")
            return False
    except:
        print("   âŒ æ£€æŸ¥å¤±è´¥")
        return False
    
    # æ£€æŸ¥ 3: Python ä¾èµ–
    print("\n3ï¸âƒ£  æ£€æŸ¥ Python ä¾èµ–...")
    
    required_packages = {
        'requests': 'HTTP è¯·æ±‚åº“',
        'numpy': 'æ•°å€¼è®¡ç®—åº“',
        'pandas': 'æ•°æ®å¤„ç†åº“'
    }
    
    missing = []
    for package, description in required_packages.items():
        try:
            __import__(package)
            print(f"   âœ… {package}")
        except ImportError:
            print(f"   âŒ {package} (ç¼ºå¤±)")
            missing.append(package)
    
    if missing:
        print(f"\n   ğŸ’¡ å®‰è£…ç¼ºå¤±çš„åŒ…:")
        print(f"   pip install {' '.join(missing)}")
        return False
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰å‰ç½®æ¡ä»¶å·²æ»¡è¶³ï¼å¯ä»¥å¼€å§‹æµ‹è¯•")
    print("="*80)
    
    return True


# ============================================================================
# å¿«é€Ÿå¯åŠ¨è„šæœ¬
# ============================================================================

def quick_start():
    """å¿«é€Ÿå¯åŠ¨æµ‹è¯•"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  å¿«é€Ÿå¯åŠ¨ Ollama Qwen æœ¬åœ°æµ‹è¯•                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ–¹å¼1ï¼šå¦‚æœä½ å·²æœ‰åˆå§‹åŒ–çš„ HippoRAG å®ä¾‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    from test_with_local_ollama import LocalOllamaRAGTest, OllamaQwenWrapper
    
    # å‡è®¾ä½ æœ‰è¿™ä¸ªå˜é‡
    rag = your_rag_instance
    
    # åˆå§‹åŒ– LLM
    llm = OllamaQwenWrapper()
    
    # åˆ›å»ºæµ‹è¯•å¯¹è±¡
    test = LocalOllamaRAGTest(rag, llm)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test.run_all_tests()
    
    # æˆ–è¿è¡Œå•ä¸ªæµ‹è¯•
    test.test_1_activation_dynamics()
    test.test_2_auto_decay()
    test.test_3_conflict_resolution()
    test.test_4_manual_cleanup()
    test.test_5_llm_integration()
    test.test_6_activation_diagnostics()


æ–¹å¼2ï¼šæœ€å°åŒ–ç¤ºä¾‹ï¼ˆå¦‚æœä½ æ²¡æœ‰ç°æˆçš„ RAGï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    from test_with_local_ollama import OllamaQwenWrapper
    
    # åˆå§‹åŒ– LLM
    llm = OllamaQwenWrapper(
        model_name="qwen3:1.7b",
        base_url="http://localhost:11434",
        temperature=0.7
    )
    
    # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    response = llm.generate("è¯·é—®ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½?", max_tokens=200)
    print(response)
    
    # æµ‹è¯•äº‹å®æå–
    text = "Erik Hort was born in Montebello, New York."
    facts = llm.extract_facts(text)
    print(facts)
    
    # æµ‹è¯•å›ç­”é—®é¢˜
    answer = llm.answer_question("Erik Hort æ˜¯è°?")
    print(answer)


æ–¹å¼3ï¼šæµ‹è¯•æ¿€æ´»çŠ¶æ€ï¼ˆä¸éœ€è¦ LLMï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    from src.hipporag import HippoRAG
    
    rag = HippoRAG(your_config)
    
    # æ·»åŠ æ–‡æ¡£
    docs = ["Document 1", "Document 2", "Document 3"]
    rag.add(docs)
    
    # æ£€ç´¢
    results = rag.retrieve("Your query")
    
    # æŸ¥çœ‹æ¿€æ´»çŠ¶æ€
    activation = rag.get_memory_activation_status("Your query")
    print(activation)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”§ Ollama æœ¬åœ°è¿è¡Œé…ç½®ï¼š

1. å¯åŠ¨ Ollama æœåŠ¡
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ollama serve
   
   (é»˜è®¤ç›‘å¬ http://localhost:11434)


2. åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‹‰å– Qwen æ¨¡å‹
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ollama pull qwen3:1.7b
   
   æˆ–å…¶ä»–å¯ç”¨çš„æ¨¡å‹:
   - ollama pull qwen3:4b
   - ollama pull mistral
   - ollama pull llama2


3. æµ‹è¯•è¿æ¥
   â”€â”€â”€â”€â”€â”€â”€â”€
   python test_with_local_ollama.py
   

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š æµ‹è¯•å¥—ä»¶è¯´æ˜ï¼š

æµ‹è¯•1: æ¿€æ´»åˆ†æ•°åŠ¨æ€å˜åŒ–
  - ç›®çš„ï¼šéªŒè¯å¤šè½®ç›¸å…³æŸ¥è¯¢ä¸­æ¿€æ´»åˆ†æ•°å¦‚ä½•å˜åŒ–
  - æœŸæœ›ï¼šç›¸å…³è®°å¿†æ¿€æ´»åº¦é«˜ï¼Œæ— å…³è®°å¿†æ¿€æ´»åº¦ä½

æµ‹è¯•2: è‡ªåŠ¨æ¶ˆé€€åŠŸèƒ½
  - ç›®çš„ï¼šéªŒè¯ä½æ¿€æ´»è®°å¿†çš„è‡ªåŠ¨åˆ é™¤
  - æœŸæœ›ï¼šä¿ç•™æ¿€æ´»åº¦é«˜çš„è®°å¿†ï¼Œåˆ é™¤ä½æ¿€æ´»çš„

æµ‹è¯•3: å†²çªæ£€æµ‹ä¸è§£å†³
  - ç›®çš„ï¼šéªŒè¯æ–°æ—§çŸ¥è¯†å†²çªæ—¶çš„å¤„ç†
  - æœŸæœ›ï¼šæ–°å€¼è¦†ç›–æ—§å€¼ï¼Œæ—§äº‹å®è¢«åˆ é™¤

æµ‹è¯•4: æ‰‹åŠ¨æ¸…é™¤åŠŸèƒ½
  - ç›®çš„ï¼šéªŒè¯ç”¨æˆ·å¯ä»¥å…ˆé¢„è§ˆååˆ é™¤
  - æœŸæœ›ï¼šdry_run=True ä»…é¢„è§ˆï¼Œdry_run=False æ‰§è¡Œåˆ é™¤

æµ‹è¯•5: Qwen LLM é›†æˆ
  - ç›®çš„ï¼šéªŒè¯å®Œæ•´çš„å¯¹è¯æµç¨‹
  - æœŸæœ›ï¼šèƒ½ç”Ÿæˆåˆç†çš„ç­”æ¡ˆ

æµ‹è¯•6: æ¿€æ´»çŠ¶æ€è¯Šæ–­
  - ç›®çš„ï¼šæ·±å…¥äº†è§£å†…å­˜æ¿€æ´»çŠ¶æ€åˆ†å¸ƒ
  - æœŸæœ›ï¼šæ˜¾ç¤ºå„ç±»å‹è®°å¿†çš„æ¿€æ´»åº¦åˆ†å¸ƒ


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ æ•…éšœæ’é™¤ï¼š

é—®é¢˜ï¼šæ— æ³•è¿æ¥åˆ° Ollama
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŸå› ï¼šOllama æœåŠ¡æœªå¯åŠ¨æˆ–éé»˜è®¤ç«¯å£
è§£å†³ï¼š
  1. è¿è¡Œ ollama serve
  2. æ£€æŸ¥é»˜è®¤ç«¯å£ http://localhost:11434
  3. å¦‚æœä½¿ç”¨å…¶ä»–ç«¯å£ï¼Œä¿®æ”¹ base_url å‚æ•°

é—®é¢˜ï¼šQwen æ¨¡å‹æœªæ‰¾åˆ°
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŸå› ï¼šæ¨¡å‹æœªä¸‹è½½æˆ–ä¸‹è½½å¤±è´¥
è§£å†³ï¼š
  1. è¿è¡Œ ollama pull qwen3:1.7b
  2. ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
  3. è¿è¡Œ ollama list æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹

é—®é¢˜ï¼šç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŸå› ï¼š1.7b çš„ Qwen æ¨¡å‹åœ¨ CPU ä¸Šç”Ÿæˆç¡®å®è¾ƒæ…¢
è§£å†³ï¼š
  1. å¦‚æœæœ‰ GPUï¼Œç¡®ä¿ ollama èƒ½ä½¿ç”¨ GPU
  2. å‡å°‘ max_tokens å‚æ•°
  3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ›´å¤§çš„æ¨¡å‹ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰

é—®é¢˜ï¼šHippoRAG åˆå§‹åŒ–å¤±è´¥
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŸå› ï¼šå¯èƒ½ç¼ºå°‘ä¾èµ–æˆ–é…ç½®é”™è¯¯
è§£å†³ï¼š
  1. æ£€æŸ¥ HippoRAG çš„æ–‡æ¡£
  2. ç¡®ä¿æ‰€éœ€çš„ embedding æ¨¡å‹å·²åŠ è½½
  3. ä¿®æ”¹ test_with_local_ollama.py ä¸­çš„åˆå§‹åŒ–ä»£ç 

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ ä¸€é”®å¯åŠ¨å‘½ä»¤ï¼š

# ç»ˆç«¯1ï¼šå¯åŠ¨ Ollama
ollama serve

# ç»ˆç«¯2ï¼šè¿è¡Œæµ‹è¯•ï¼ˆç¡®ä¿å·²ä¿®æ”¹ test_with_local_ollama.py ä¸­çš„ HippoRAG åˆå§‹åŒ–ï¼‰
python test_with_local_ollama.py
    """)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--check":
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        check_prerequisites()
    else:
        # æ˜¾ç¤ºå¿«é€Ÿå¯åŠ¨æŒ‡å—
        quick_start()
